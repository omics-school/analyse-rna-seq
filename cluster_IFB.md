---
title: Calcul sur le cluster de l'IFB
author: Pierre Poulain
license: Creative Commons Attribution-ShareAlike (CC BY-SA 4.0)
---


Dans cette activit√©, vous allez analyser des donn√©es RNA-seq de *O. tauri* avec le cluster *National Network of Computational Resources* (NNCR) de l'[Institut Fran√ßais de Bioinformatique]((https://www.france-bioinformatique.fr/) (IFB). Ce cluster utilise un syst√®me d'exploitation Linux.


## Remarques pr√©ables

L'acc√®s au cluster de l'IFB vous est fourni dans le cadre p√©dagogique du DU Omiques. Cet acc√®s sera r√©voqu√© √† l'issue de la formation. 

Si, √† l'issue de cette formation, vous souhaitez continuer √† utiliser ce cluster pour votre projet de recherche, connectez-vous √† l'[interface de gestion de votre compte IFB](https://my.cluster.france-bioinformatique.fr/manager2/project) puis cliquez sur le bouton *Request A New Project* et d√©taillez en quelques mots votre projet. Plusieurs utilisateurs peuvent √™tre associ√©es √† un m√™me projet et partager des donn√©es. Selon la quantit√© de ressources que vous demanderez, la cr√©ation d'un projet pourra √™tre associ√©e √† un co√ªt. Au 06/01/2022, la grille tarifaire n'est pas encore connue.

Si vous avez besoin d'un logiciel sp√©cifique sur le cluster. N'h√©sitez pas √† le demander gentillement sur le site [Cluster Community Support](https://community.france-bioinformatique.fr/c/ifb-core-cluster/). Les administrateurs sont tr√®s sympas et en g√©n√©ral tr√®s r√©actifs.


## 0. Connexion au cluster

Sous Windows, ouvrez un terminal Ubuntu. Si vous avez oubli√© comment faire, consultez le [tutoriel Unix](https://omics-school.github.io/unix-tutorial/tutoriel/README).

Connectez-vous en SSH au cluster avec les identifiants (login et mot de passe) que vous avez du recevoir par e-mail.

La syntaxe est de la forme :

```bash
$ ssh LOGIN@core.cluster.france-bioinformatique.fr
```

avec `LOGIN` votre identifiant sur le cluster. 

üîî Rappels : 

- Ne tapez pas le `$` en d√©but de ligne et faites attention aux majuscules et aux minuscules !
- Utilisez le copier / coller.
- Utilisez la compl√©tion des noms de fichier et de r√©pertoires avec la touche <kbd>Tab</kbd>.

Si c'est la premi√®re fois que vous vous connectez au cluster, r√©pondez `yes` √† la question 

```
Are you sure you want to continue connecting (yes/no)?
```

puis validez.

Entrez ensuite votre mot de passe en **aveugle**, c'est-√†-dire sans qu'aucun caract√®re ne soit affich√© √† l'√©cran. C'est assez d√©stabilisant la premi√®re fois puis on s'habitue.

Si vous le souhaitez, une [vid√©o](https://youtu.be/7OnrlZbVtEk) illustrant pas-√†-pas la connexion en SSH est disponible.

üîî **Attention** üîî Le cluster est prot√©g√© contre certaines attaques. Si vous entrez un mot de passe erronn√© plusieurs fois de suite, votre adresse IP va √™tre bannie et vous ne pourrez plus vous connecter (temporairement) au serveur.

Pour vous d√©connecter du cluster et revenir √† votre terminal local, pressez la combinaison de touches <kbd>Ctrl</kbd>+<kbd>D</kbd>.

Un cluster est un ensemble de machines. La machine √† laquelle vous venez de vous connecter est le noeud de connexion. C'est aussi depuis cette machine que vous lancerez vos analyses. 

**Remarque :** Vous lancerez vos calculs **depuis** le noeud de connexion mais pas **sur** le noeud de connexion. Il est interdit de lancer une analyse sur le noeud de connexion sous peine de voir votre compte suspendu.


## 1. Environnement logiciel 

Si vous vous √™tes d√©connect√©s du cluster, reconnectez-vous avec la commande `ssh` pr√©c√©dente.

Par d√©faut, aucun logiciel de bioinformatique n'est pr√©sent. Pour vous en convaincre, essayez de lancer la commande :

```bash
$ bowtie2 --version
```
Vous devriez obtenir un message d'erreur du type : `-bash: bowtie2 : commande introuvable`

Chaque logiciel doit donc √™tre charg√© individuellement avec l'outil `module`.

Utilisez la commande suivante pour compter le nombre de logiciels et de versions disponibles avec `module` :

```bash
$ module avail -l | wc -l
```

V√©rifiez maintenant si la version 2.3.6 de `bowtie2` est disponible avec la commande :

```bash
$ module avail -l bowtie2
```

Si un jour vous avez besoin d'un logiciel dans une version sp√©cifique, n'h√©sitez pas √† le demander au [support communautaire](https://community.france-bioinformatique.fr/c/ifb-core-cluster/) du cluster.

Chargez ensuite les logiciels `fastqc`, `bowtie2`, `samtools` et `htseq` avec les commandes suivantes :

```bash
$ module load fastqc/0.11.9
$ module load bowtie2/2.3.5
$ module load samtools/1.9
$ module load htseq/0.11.3
```

**Remarque** : les logiciels charg√©s avec `module` ne sont disponibles que le temps de votre session sur le cluster. Si vous vous d√©connectez puis vous vous reconnectez, il faudra charger √† nouveau les logiciels dont vous aurez besoin.


V√©rifiez que les logiciels sont bien disponibles en affichant leurs versions :

```bash
$ fastqc --version
FastQC v0.11.9
```

```bash
$ bowtie2 --version
/home/duo/miniconda3/envs/rnaseq/bin/bowtie2-align-s version 2.3.5.1
64-bit
Built on
Wed Apr 17 02:40:25 UTC 2019
[...]
```

```bash
$ samtools --version
samtools 1.9
Using htslib 1.9
Copyright (C) 2018 Genome Research Ltd.
```

```bash
$ htseq-count -h | tail -n 4
Written by Simon Anders (sanders@fs.tum.de), European Molecular Biology
Laboratory (EMBL) and Fabio Zanini (fabio.zanini@stanford.edu), Stanford
University. (c) 2010-2019. Released under the terms of the GNU General Public
License v3. Part of the 'HTSeq' framework, version 0.11.3.
```

## 2. Stockage des donn√©es

Votre r√©pertoire utilisateur sur le noeud de connexion est : `/shared/home/LOGIN` (avec `LOGIN` votre identifiant sur le cluster).

Ce r√©pertoire ne doit pas contenir de donn√©e volumineuse car l'espace disponible est limit√© √† 100 Go. Un espace de stockage plus cons√©quent a √©t√© cr√©√© pour vous dans le r√©pertoire  `/shared/projects/form_2021_29/LOGIN` . Par la suite, cet espace sera appel√© ¬´ r√©pertoire de travail ¬ª.

De plus, le r√©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri` contient les donn√©es dont vous aurez besoin pour ce projet. Vous n'avez acc√®s √† ce r√©pertoire qu'en lecture seule, c'est-√†-dire que vous pouvez seulement parcourir les r√©pertoires et lire les fichiers de ce r√©pertoire (pas de modification, d'ajout ou de suppression).

Essayez de vous souvenir de quels fichiers vous aurez besoin pour l'analyse des donn√©es RNA-seq de *O. tauri*. Trouvez-vous ces fichiers dans le r√©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri` ou un de ses sous-r√©pertoires ?

V√©rifiez l'int√©grit√© des fichiers `.fastq.gz` situ√©s dans le r√©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri/reads` avec les commandes suivantes :

```bash
$ cd /shared/projects/form_2021_29/data/rnaseq_tauri
```

*Rappel : N'entrez pas le symbole $ en d√©but de ligne.*

puis :

```bash
$ srun -A form_2021_29 md5sum -c reads_md5sum.txt
```

N'oubliez pas `srun -A form_2021_29` en d√©but de commande :

- L'instruction `srun` est sp√©cifique au cluster. 
- L'option `-A form_2021_29` sp√©cifie quel projet utiliser (facturer) pour cette commande. Un m√™me utilisateur peut appartenir √† plusieurs projets. Le nombre d'heures de calcul attribu√©es √† un projet √©tant limit√©, il est important de savoir quel projet imputer pour telle ou telle commande. Pensez-y pour vos futurs projets.


D√©placez-vous maintenant dans votre r√©pertoire de travail `/shared/projects/form_2021_29/LOGIN` (avec `LOGIN` votre identifiant sur le cluster). Un moyen simple d'y parvenir est d'ex√©cuter la commande :

```bash
$ cd /shared/projects/form_2021_29/$USER
```

Cr√©ez ensuite le r√©pertoire `rnaseq_tauri` et d√©placez-vous √† l'int√©rieur. Dor√©navant vous ne travaillerez plus qu'√† partir de ce r√©pertoire.

La commande `pwd` devrait vous renvoyer quelque chose du type :

```bash
$ pwd
/shared/projects/form_2021_29/LOGIN/rnaseq_tauri`
```

avec `LOGIN` votre identifiant sur le cluster.

üõë Si vous ne parvenez pas √† √™tre dans le bon r√©pertoire, n'allez pas plus loin et appelez √† l'aide üÜò.


## 3.1 Analyse d'un seul √©chantillon

En guise d'introduction, vous affichez tous les jobs en cours d'ex√©cution sur le cluster avec la commande :

```bash
$ squeue -t RUNNING
```

Vous voyez que vous n'√™tes pas seul ! Comptez maintenant le nombre de jobs en cours d'ex√©cution en cha√Ænant la commande pr√©c√©dente avec `wc -l` :

```bash
$ squeue -t RUNNING | wc -l
```


**Remarques importantes concernant l'indexation du g√©nome de r√©f√©rence** : 

- L'indexation du g√©nome de r√©f√©rence avec le logiciel `bowtie2` a d√©j√† √©t√© effectu√© pour vous. Pour vous en convraincre, affichez le contenu du r√©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri/genome` et v√©rifiez l'existence de fichiers avec l'extension `.bt2`, sp√©cifiques des fichiers index cr√©√©s par `bowtie2`.
- Cette indexation a √©t√© r√©alis√©e avec la commande `sbatch -A form_2021_29 /shared/projects/form_2021_29/data/rnaseq_tauri/build_genome_index.sh` que, bien s√ªr, vous n'ex√©cuterez pas !

Depuis le cluster de l'IFB, v√©rifiez que vous √™tes toujours dans votre r√©pertoire `/shared/projects/form_2021_29/LOGIN/rnaseq_tauri`.

T√©l√©chargez ensuite le script `script4.sh` avec la commande :

```bash
$ wget https://raw.githubusercontent.com/omics-school/analyse-rna-seq/master/script4.sh
```

Pour en comprendre le fonctionnement, explorez le contenu de ce script avec la commande `less` :

```bash
$ less script4.sh
```

Utilisez les touches <kbd>‚Üì</kbd> et <kbd>‚Üë</kbd> pour naviguer dans le fichier. Pressez la touche <kbd>Q</kbd> pour quitter `less`.

Lancez ensuite ce script avec la commande :

```bash
$ sbatch -A form_2021_29 script4.sh
```

Vous devriez obtenir un message du type `Submitted batch job 20716345`. Ici, `20716345` correspond au num√©ro du job.

Notez bien le num√©ro de votre job.

V√©rifiez que votre script est en train de tourner avec la commande :

```bash
$ squeue -u $USER
```

**Remarque** : Voici quelques statuts (colonne `ST`) de job int√©ressants :

- `CA` (*cancelled*) : le job a √©t√© annul√©
- `F` (*failed*) : le job a plant√©
- `PD` (*pending*) : le job est en attente que des ressources soient disponibles
- `R` (*running*) : le job est en cours d'ex√©cution


Et pour avoir plus de d√©tails, utilisez la commande :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```
avec `JOBID` (en fin de ligne) le num√©ro de votre job √† remplacer par le v√¥tre.

Voici un exemple de sortie que vous pourriez obtenir :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j 20716345
       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList 
------------ ---------- ---------- ------------------- ---------- ---------- --------------- 
20716345     script4.sh    RUNNING 2022-01-05T23:37:36   00:08:58   00:08:58     cpu-node-24 
20716345.ba+      batch    RUNNING 2022-01-05T23:37:36   00:08:58   00:08:58     cpu-node-24 
20716345.0       fastqc    RUNNING 2022-01-05T23:37:37   00:00:54   00:00:34     cpu-node-24 
```

Si vous affichez le contenu de votre r√©pertoire courant, vous devriez observer un fichier `slurm-JOBID.out` o√π `JOBID` est le num√©ro de votre job. Ce fichier contient la sortie, c'est-√†-dire le *log* de votre script.

Affichez son contenu avec la commande `cat`. Par exemple :

```bash
$ cat slurm-JOBID.out
```

avec `JOBID` le num√©ro de votre job.

Si vous attendez 1 ou 2 minutes et relancez la commande `sacct` pr√©c√©dente, votre job a du passer √† une nouvelle √©tape.

Par exemple :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j 20716345
       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList 
------------ ---------- ---------- ------------------- ---------- ---------- --------------- 
20716345     script4.sh    RUNNING 2022-01-05T23:37:36   00:05:22   00:05:22     cpu-node-24 
20716345.ba+      batch    RUNNING 2022-01-05T23:37:36   00:05:22   00:05:22     cpu-node-24 
20716345.0       fastqc  COMPLETED 2022-01-05T23:37:37   00:00:54   00:00:54     cpu-node-24 
20716345.1      bowtie2    RUNNING 2022-01-05T23:38:31   00:04:27   00:04:27     cpu-node-24 
```

Nous allons maintenant am√©liorer le script d'analyse. Annulez votre job en cours avec la commande :

```bash
$ scancel JOBID
```

o√π `JOBID` est le num√©ro de votre job.

Faites aussi un peu de m√©nage en supprimant les fichiers cr√©√©s pr√©c√©demment :

```bash
$ rm -rf map/ reads_qc/ count/ slurm*.out
```

## 3.2 Analyse plus rapide d'un √©chantillon

L'objectif est maintenant ¬´ d'aller plus vite ¬ª en attribuant plusieurs coeurs pour l'√©tape d'alignement des reads sur le g√©nome avec `bowtie2`.

Toujours depuis le cluster de l'IFB, dans le r√©pertoire `rnaseq_tauri` de votre r√©pertoire de travail, t√©l√©chargez le script `script5.sh` avec la commande :

```bash
$ wget https://raw.githubusercontent.com/omics-school/analyse-rna-seq/master/script5.sh
```

Identifiez les diff√©rences avec le script pr√©c√©dent avec la commande `diff` :

```bash
$ diff script4.sh script5.sh
```

Les lignes qui d√©butent par `<` viennent de `script4.sh` et celles qui d√©butent par `>` viennent de `script5.sh`.

La diff√©rence majeure avec `script4.sh` r√©side dans l'utilisation de plusieurs coeurs pour la commande `bowtie2` avec l'option `--threads="${SLURM_CPUS_PER_TASK}"`. L'utilisation de plusieurs coeurs est permise par la d√©claration `#SBATCH --cpus-per-task=8` au tout d√©but de `script5.sh`.

**Remarque** : nous aurions √©galement pu attribuer plusieurs coeurs pour les commandes `samtools view` et `samtools sort`, mais nos tests ont montr√© qu'il n'y avait pas, pour ce cas pr√©cis, de gain significatif en terme de temps de calcul. Pour information, les lignes de commande √† utiliser seraient :

```bash
srun samtools view --threads="${SLURM_CPUS_PER_TASK}" -b "map/bowtie-${sample}.sam" -o "map/bowtie-${sample}.bam"
srun samtools sort --threads="${SLURM_CPUS_PER_TASK}" "map/bowtie-${sample}.bam" -o "map/bowtie-${sample}.sorted.bam"
```

Notez que tous les logiciels ne peuvent utiliser plusieurs coeurs. Consultez toujours la documentation de l'outil consid√©r√©.

Lancez maintenant le script d'analyse `script5.sh` :

```bash
$ sbatch -A form_2021_29 script5.sh
```

Notez bien le num√©ro de job renvoy√©.

V√©rifiez que votre job est bien lanc√© avec la commande :
```bash
$ squeue -u $USER
```

Le fichier `slurm-JOBID.out` est √©galement cr√©√© et contient les sorties du script. Pour consulter son contenu, tapez :

```bash
$ cat slurm-JOBID.out
```

avec `JOBID` le num√©ro de votre job.


Suivez √©galement en temps r√©el l'ex√©cution de votre job avec la commande :

```bash
$ watch sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```
avec `JOBID` le num√©ro de votre job.

Remarques : 

- La commande `watch` est utilis√©e ici pour ¬´ surveiller ¬ª en quasi-temps r√©el le r√©sultat de la commande `sacct`.
- L'affichage est rafraichi toutes les 2 secondes.
- Vous pouvez √©galement afficher la m√©moire vive maximale consomm√©e √† chaque √©tape avec la commande `sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,MaxRSS,NodeList -j JOBID`

Votre job devrait prendre une dizaine de minutes pour s'ex√©cuter. Laissez le cluster travailler et profitez-en pour vous pr√©parer un th√© ou un caf√© bien m√©rit√©.

Quand les statuts (colonne `State`) du job et de tous ses ¬´ *job steps* ¬ª sont √† `COMPLETED`, quittez la commande `watch` en appuyant sur la combinaison de touches <kbd>Ctrl</kbd> + <kbd>C</kbd>.

V√©rifiez avec la commande `tree` que vous obtenez une arborescence √©quivalente √† celle ci-dessous :

```
$ tree
.
‚îú‚îÄ‚îÄ count
‚îÇ   ‚îî‚îÄ‚îÄ count-SRR2960338.txt
‚îú‚îÄ‚îÄ map
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960338.sorted.bam
‚îÇ   ‚îî‚îÄ‚îÄ bowtie-SRR2960338.sorted.bam.bai
‚îú‚îÄ‚îÄ reads_qc
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960338_fastqc.html
‚îÇ   ‚îî‚îÄ‚îÄ SRR2960338_fastqc.zip
‚îú‚îÄ‚îÄ script4.sh
‚îú‚îÄ‚îÄ script5.sh
‚îî‚îÄ‚îÄ slurm-20716384.out
```

V√©rifiez que la somme de contr√¥le du fichier `count/count-SRR2960338.txt` est bien `36fc86a522ee152c89fd77430e9b56a5`.

Faites maintenant un peu de m√©nage en supprimant les fichiers cr√©√©s pr√©c√©demment avec la commande :

```bash
$ rm -rf map/ reads_qc/ count/ slurm*.out
```


## 3.3 Analyse de plusieurs √©chantillons

Toujours depuis le cluster de l'IFB, dans le r√©pertoire `rnaseq_tauri` de votre r√©pertoire de travail, t√©l√©chargez le script `script6.sh` avec la commande :

```bash
$ wget https://raw.githubusercontent.com/omics-school/analyse-rna-seq/master/script6.sh
```

Nous pourrions analyser d'un seul coup les 47 √©chantillons (fichiers `.fastq.gz`) mais pour ne pas consommer trop de ressources sur le cluster, nous allons limiter notre analyse √† 4 √©chantillons seulement. Si vous le souhaitez vous pourrez modifier ce script pour analyser les 47 √©chantillons üí™.

Lancez votre analyse avec la commande :

```bash
$ sbatch -A form_2021_29 script6.sh
```

Notez bien le num√©ro de job renvoy√©.

Suivez en temps r√©el l'ex√©cution de votre job avec la commande :

```bash
$ watch sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```
avec `JOBID` le num√©ro de votre job.

Remarquez que la ligne indiquant `script6.sh` pour ¬´ *JobName* ¬ª est pr√©sente 4 fois. Cela indique que le script `script6.sh` est ex√©cut√©e 4 fois, en parall√®le.

Patientez une dizaine de minutes que tous les jobs et *job steps* soient termin√©s. 

Quand les status (colonne `State`) de tous les jobs et *job steps* sont √† `COMPLETED`, quittez la commande `watch` en appuyant sur la combinaison de touches <kbd>Ctrl</kbd> + <kbd>C</kbd>.

**Remarques**: 

- Notez que l'ex√©cution de `script6.sh` aura pris environ le m√™me temps que celle de `script5.sh`. C'est toute la puissance du calcul distribu√© üöÄ. 
- Vous comprenez qu'il est possible d'analyser 4, 10 ou 47 √©chantillons dans un temps raisonnable. Cependant, si vous lancez une telle analyse sur un grand nombre d'√©chantillons, il est possible que tous vos jobs ne partent pas en m√™me temps et que certains aient le statut *PENDING*, le temps que les ressources n√©cessairent se lib√®rent sur le cluster.
- L'analyse de la totalit√© des 47 √©chantillons (fichiers .fastq.gz) g√©n√®re environ 18 Go de donn√©es.


Une derni√®re fois, v√©rifiez que tous vos fichiers sont pr√©sents dans les bons r√©pertoires :

```bash
$ tree
.
‚îú‚îÄ‚îÄ count
‚îÇ   ‚îú‚îÄ‚îÄ count-SRR2960338.txt
‚îÇ   ‚îú‚îÄ‚îÄ count-SRR2960341.txt
‚îÇ   ‚îú‚îÄ‚îÄ count-SRR2960343.txt
‚îÇ   ‚îî‚îÄ‚îÄ count-SRR2960356.txt
‚îú‚îÄ‚îÄ map
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960338.sorted.bam
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960338.sorted.bam.bai
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960341.sorted.bam
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960341.sorted.bam.bai
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960343.sorted.bam
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960343.sorted.bam.bai
‚îÇ   ‚îú‚îÄ‚îÄ bowtie-SRR2960356.sorted.bam
‚îÇ   ‚îî‚îÄ‚îÄ bowtie-SRR2960356.sorted.bam.bai
‚îú‚îÄ‚îÄ reads_qc
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960338_fastqc.html
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960338_fastqc.zip
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960341_fastqc.html
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960341_fastqc.zip
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960343_fastqc.html
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960343_fastqc.zip
‚îÇ   ‚îú‚îÄ‚îÄ SRR2960356_fastqc.html
‚îÇ   ‚îî‚îÄ‚îÄ SRR2960356_fastqc.zip
‚îú‚îÄ‚îÄ script4.sh
‚îú‚îÄ‚îÄ script5.sh
‚îú‚îÄ‚îÄ script6.sh
‚îú‚îÄ‚îÄ slurm-20716400_0.out
‚îú‚îÄ‚îÄ slurm-20716400_1.out
‚îú‚îÄ‚îÄ slurm-20716400_2.out
‚îî‚îÄ‚îÄ slurm-20716400_3.out
```

Comme vous avez lanc√© 4 sous-jobs ind√©pendants, SLURM a √©galement cr√©√© 4 fichiers de sortie distincts.

## 4. L'heure de faire les comptes

Exp√©rimentez la commande `sreport` pour avoir une id√©e du temps de calcul consomm√© par tous vos jobs :

```bash
$ sreport -t hour Cluster UserUtilizationByAccount Start=2022-01-01 End=$(date --iso-8601)T23:59:59 Users=$USER
```

La colonne `Used` indique le nombre d'heures de temps CPU consomm√©es. Cette valeur est utile pour estimer le ¬´ co√ªt CPU ¬ª d'un projet.

Voici un exemple de rapport produit par `sreport` :

```bash
$ sreport -t hour Cluster UserUtilizationByAccount Start=2022-01-01 End=$(date --iso-8601)T23:59:59 Users=$USER
--------------------------------------------------------------------------------
Cluster/User/Account Utilization 2022-01-01T00:00:00 - 2022-01-06T17:59:59 (496800 secs)
Usage reported in CPU Hours
--------------------------------------------------------------------------------
  Cluster     Login     Proper Name         Account     Used   Energy 
--------- --------- --------------- --------------- -------- -------- 
     core  ppoulain  Pierre Poulain    form_2021_29       93        0 
     core  ppoulain  Pierre Poulain          gonseq        5        0 
```

Ainsi, l'utilisateur `ppoulain` a d√©j√† consomm√© 93 heures de temps CPU sur le projet `form_2021_29`.

Attention, `sreport` ne prend pas en compte les heures imm√©diatement consomm√©es. Il lui faut un peu de temps pour consolider les donn√©es.


## 5. Quelques conseils suppl√©mentaires

L'analyse RNA-seq pr√©sent√©e ici tourne en 10-15', c'est tr√®s rapide car le g√©nome d'*O. tauri* est relativement petit. Les temps d'analyse seront plus longs avec des g√©nomes plus gros.

Proc√©dez toujours par it√©rations successives. Testez votre script d'analyse RNA-seq pour 1 √©chantillon, puis 2 ou 3 puis la totalit√©.

Quand vous lancez un job qui sera potentiellement long, n'h√©sitez pas √† ajouter les directives ci-dessous au d√©but de votre script :

```
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=votre-adresse-mail@email.fr
```

Vous recevrez alors automatiquement un e-mail lorsque le job se termine ou si celui-ci plante.

Sur le cluster IFB :

- Un utilisateur ne peut utiliser plus de 300 coeurs en m√™me temps.
- Un job dure, par d√©faut, au maximum 24 h. Une queue plus longue (appel√©e `long`) est disponible pour des jobs qui durent jusqu'√† 30 jours et est utilisable via la directive `#SBATCH --partition=long` en d√©but de script. D'autres queues plus sp√©cifiques (plus de temps, beaucoup de m√©moire vive, GPU) sont disponibles sur demande.

Prenez le temps d'explorer la [documentation tr√®s compl√®te](https://ifb-elixirfr.gitlab.io/cluster/doc/) sur le cluster. Vous y trouverez notamment un [tutoriel](https://ifb-elixirfr.gitlab.io/cluster/doc/tutorials/analysis_slurm/) sur une autre analyse RNA-seq.


## 6. Sur place ou √† emporter ?

Nous allons maintenant utiliser deux strat√©gies pour r√©cup√©rer les r√©sultats de l'analyse RNA-seq et copier les donn√©es depuis le cluster vers notre machine locale.

Une [vid√©o](https://youtu.be/1UTPzK-zeA8) illustrant ces deux approches est √©galement disponible.

### 6.1 scp

‚ö†Ô∏è Pour r√©cup√©rer des fichiers sur le cluster en ligne de commande, vous devez lancer la commande `scp` depuis un shell Unix sur votre machine locale. ‚ö†Ô∏è

Depuis un shell Unix sur votre machine locale, d√©placez-vous dans le r√©pertoire `/mnt/c/Users/omics` et cr√©ez le r√©pertoire `rnaseq_tauri_cluster`. 

D√©placez-vous dans ce nouveau r√©pertoire.

Utilisez la commande `pwd` pour v√©rifier que vous √™tes bien dans le r√©pertoire `/mnt/c/Users/omics/rnaseq_tauri_cluster`. 

Lancez ensuite la commande suivante pour r√©cup√©rer les fichiers de comptage :

```bash
$ scp LOGIN@core.cluster.france-bioinformatique.fr:/shared/projects/form_2021_29/LOGIN/rnaseq_tauri/count/count*.txt .
```

o√π `LOGIN` est votre identifiant sur le cluster (qui apparait deux fois dans la ligne de commande ci-dessus). Faites bien attention √† garder le `.` √† la fin de la ligne de commande.

Comme d'habitude, entrez votre mot de passe du cluster en aveugle.

Une fois que vous avez r√©cup√©r√© les r√©sultats de comptage, v√©rifiez que la somme de contr√¥le MD5 du fichier `count-SRR2960338.txt` est bien la m√™me que pr√©c√©demment (`36fc86a522ee152c89fd77430e9b56a5`).

Pour r√©cup√©rer directement le r√©pertoire `count` sur le cluster, vous auriez pu utiliser la commande :

```bash
$ scp -r LOGIN@core.cluster.france-bioinformatique.fr:/shared/projects/form_2021_29/LOGIN/rnaseq_tauri/count .
```

Notez l'option `-r` qui indique qu'on transf√®re un r√©pertoire.

### 6.2 FileZilla

Lancez le logiciel FileZilla ([comme ceci](img/filezilla.png)). Puis entrez les informations suivantes :

- H√¥te : `sftp://core.cluster.france-bioinformatique.fr`
- Identifiant : votre login sur le cluster
- Mot de passe : votre mot de passe sur le cluster

Cliquez ensuite sur le bouton *Connexion rapide*. Cliquez sur *OK* dans la fen√™tre *Cl√© de l'h√¥te inconnue*

Une fois connect√© :

- Dans le champ texte √† c√¥t√© de *Site local* (√† gauche de la fen√™tre), entrez le chemin `C:\Users\omics\rnaseq_tauri_cluster`.
- Dans le champ texte √† cot√© de *Site distant* (√† droite de la fen√™tre), entrez le chemin `/shared/projects/form_2021_29/` voire directement votre r√©pertoire de travail `/shared/projects/form_2021_29/LOGIN/` (avec `LOGIN` votre identifiant sur le cluster).

Essayez de transf√©rer des fichiers dans un sens puis dans l'autre. Double-cliquez sur les fichiers pour lancer les transferts.



