---
title: Calcul sur le cluster de l'IFB
author: Pierre Poulain
license: Creative Commons Attribution-ShareAlike (CC BY-SA 4.0)
---


Dans cette activitÃ©, vous allez analyser les donnÃ©es RNA-seq de *O. tauri* avec le cluster *National Network of Computational Resources* (NNCR) de l'Institut FranÃ§ais de Bioinformatique (IFB). Ce cluster utilise un systÃ¨me d'exploitation Linux.


## Remarques prÃ©ables

L'accÃ¨s au cluster de l'IFB vous est fourni dans le cadre du DU Omiques. Cet accÃ¨s sera rÃ©voquÃ© Ã  l'issue de la formation. 

Si, Ã  l'issue de cette formation, vous souhaitez continuer Ã  utiliser ce cluster pour votre projet de recherche, connectez-vous sur votre [interface](https://my.cluster.france-bioinformatique.fr/manager2/project) puis cliquez sur le bouton *Request A New Project* et prÃ©cisez en quelques mots votre projet. Plusieurs utilisateurs peuvent Ãªtre associÃ©es Ã  un mÃªme projet et partager des donnÃ©es. Selon la quantitÃ© de ressources que vous demanderez, la crÃ©ation d'un projet pourra Ãªtre associÃ©e Ã  un coÃ»t. Au 06/01/2022, la grille tarifaire n'est pas encore connue.

Si vous avez besoin d'un logiciel spÃ©cifique sur le cluster. N'hÃ©sitez pas Ã  le demander gentillement sur le site [Cluster Community Support](https://community.cluster.france-bioinformatique.fr/). Les administrateurs sont en gÃ©nÃ©ral trÃ¨s rÃ©actifs.


## 0. Connexion au cluster

Sous Windows, ouvrez un terminal Ubuntu. Si vous avez oubliÃ© comment faire, consultez le [tutoriel Unix](https://omics-school.github.io/unix-tutorial/tutoriel/README).

DÃ©placez vous ensuite dans le rÃ©pertoire `/mnt/c/Users/omics` :

```bash
$ cd /mnt/c/Users/omics
```

ğŸ”” Rappels : 

- Ne tapez pas le `$` en dÃ©but de ligne et faites attention aux majuscules et aux minuscules (surtout pour `Users`) !
- Utilisez le copier / coller.
- Utilisez la complÃ©tion des noms de fichier et de rÃ©pertoires avec la touche <kbd>Tab</kbd>.

Connectez-vous en SSH au cluster avec les identifiants (login et mot de passe) que vous avez du recevoir par e-mail.

La syntaxe est de la forme :
```bash
$ ssh LOGIN@core.cluster.france-bioinformatique.fr
```

avec `LOGIN` votre identifiant sur le cluster. 

Si c'est la premiÃ¨re fois que vous vous connectez au cluster, rÃ©pondez `yes` Ã  la question 
```
Are you sure you want to continue connecting (yes/no)?
```

Entrez ensuite votre mot de passe en **aveugle**, c'est-Ã -dire sans qu'aucun caractÃ¨re ne soit affichÃ© Ã  l'Ã©cran. C'est assez dÃ©stabilisant la premiÃ¨re fois puis on s'habitue.

ğŸ”” **Attention** ğŸ”” Le cluster est protÃ©gÃ© contre certaines attaques. Si vous entrez un mot de passe erronnÃ© plusieurs fois de suite, votre adresse IP va Ãªtre bannie et vous ne pourrez plus vous connecter (temporairement) au serveur.


Pour vous dÃ©connecter du cluster et revenir Ã  votre terminal local, pressez la combinaison de touches <kbd>Ctrl</kbd>+<kbd>D</kbd>.

Un cluster est un ensemble de machines. La machine Ã  laquelle vous venez de vous connecter est le noeud de connexion. C'est aussi depuis cette machine que vous lancerez vos analyses. 

**Remarque :** Vous lancerez vos calculs **depuis** le noeud de connexion mais pas **sur** le noeud de connexion. Il est interdit de lancer une analyse sur le noeud de connexion sous peine de voir votre compte suspendu.


## 1. Environnement logiciel 

Si vous vous Ãªtes dÃ©connectÃ©s du cluster, reconnectez-vous avec la commande `ssh` prÃ©cÃ©dente.

Par dÃ©faut, aucun logiciel de bioinformatique n'est prÃ©sent. Pour vous en convaincre, essayez de lancer la commande :

```bash
$ bowtie2 --version
```
Vous devriez obtenir un message d'erreur du type : `-bash: bowtie2 : commande introuvable`

Chaque logiciel doit donc Ãªtre chargÃ© individuellement avec l'outil `module`.

Utilisez la commande suivante pour compter le nombre de logiciels et de versions disponibles avec `module` :

```bash
$ module avail -l | wc -l
```

VÃ©rifiez maintenant si la version 2.3.6 de `bowtie2` est disponible avec la commande :

```bash
$ module avail -l bowtie2
```

Si un jour vous avez besoin d'un logiciel dans une version spÃ©cifique, n'hÃ©sitez pas Ã  le demander au [support communautaire](https://community.france-bioinformatique.fr/c/ifb-core-cluster/) du cluster.

Chargez ensuite les logiciels `fastqc`, `bowtie2`, `samtools` et `htseq` avec les commandes suivantes :

```bash
$ module load fastqc/0.11.9
$ module load bowtie2/2.3.5
$ module load samtools/1.9
$ module load htseq/0.11.3
```

VÃ©rifiez que les logiciels sont bien disponibles en affichant leurs versions :

```bash
$ fastqc --version
FastQC v0.11.9
```

```bash
$ bowtie2 --version
/home/duo/miniconda3/envs/rnaseq/bin/bowtie2-align-s version 2.3.5.1
64-bit
Built on
Wed Apr 17 02:40:25 UTC 2019
[...]
```

```bash
$ samtools --version
samtools 1.9
Using htslib 1.9
Copyright (C) 2018 Genome Research Ltd.
```

```bash
$ htseq-count -h | tail -n 4
Written by Simon Anders (sanders@fs.tum.de), European Molecular Biology
Laboratory (EMBL) and Fabio Zanini (fabio.zanini@stanford.edu), Stanford
University. (c) 2010-2019. Released under the terms of the GNU General Public
License v3. Part of the 'HTSeq' framework, version 0.11.3.
```

## 2. Stockage des donnÃ©es

Votre rÃ©pertoire utilisateur sur le noeud de connexion est : `/shared/home/LOGIN` (avec `LOGIN` votre identifiant sur le cluster).

Ce rÃ©pertoire ne doit pas contenir de donnÃ©e volumineuse car l'espace disponible est limitÃ© Ã  100 Go. Un espace de stockage a Ã©tÃ© crÃ©Ã© pour vous dans le rÃ©pertoire  `/shared/projects/form_2021_29/LOGIN` . Par la suite, cet espace sera appelÃ© Â« rÃ©pertoire de travail Â».

De plus, le rÃ©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri` contient les donnÃ©es dont vous aurez besoin pour ce projet. Vous n'avez accÃ¨s Ã  ce rÃ©pertoire qu'en lecture, c'est-Ã -dire que vous pouvez seulement parcourir les rÃ©pertoires et lire les fichiers de ce rÃ©pertoire (pas de modification, d'ajout ou de suppression).

De quels fichiers avez-vous besoin pour l'analyse des donnÃ©es RNA-seq de *O. tauri* ? 

VÃ©rifiez que tous les fichiers nÃ©cessaires sont bien prÃ©sents dans `/shared/projects/form_2021_29/data/rnaseq_tauri`.

VÃ©rifiez l'intÃ©gritÃ© des fichiers `.fastq.gz` situÃ©s dans le rÃ©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri/reads` avec les commandes suivantes :

```bash
$ cd /shared/projects/form_2021_29/data/rnaseq_tauri
```

*Rappel : n'entrez pas le symbole $ en dÃ©but de ligne*

puis 

```bash
$ srun -A form_2021_29 md5sum -c reads_md5sum.txt
```

N'oubliez pas le `srun -A form_2021_29` en dÃ©but de commande :

- L'instruction `srun` est spÃ©cifique au cluster. 
- L'option `-A form_2021_29` spÃ©cifie quel projet utiliser (facturer) pour cette commande. Un mÃªme utilisateur peut appartenir Ã  plusieurs projets. Le nombre d'heures de calcul attribuÃ©es Ã  un projet Ã©tant limitÃ©, il est important de savoir quel projet imputÃ© pour telle ou telle commande. Pensez-y pour vos futurs projets.


DÃ©placez-vous maintenant dans votre rÃ©pertoire de travail `/shared/projects/form_2021_29/LOGIN` (avec `LOGIN` votre identifiant sur le cluster).

CrÃ©ez le rÃ©pertoire `rnaseq_tauri` et dÃ©placez-vous Ã  l'intÃ©rieur. DorÃ©navant vous ne travaillerez plus qu'Ã  partir de ce rÃ©pertoire.

La commande `pwd` devrait vous renvoyer quelque chose du type :

```bash
$ pwd
/shared/projects/form_2021_29/LOGIN/rnaseq_tauri`
```

avec `LOGIN` votre identifiant sur le cluster. ğŸ†˜ Appelez Ã  l'aide si vous ne parvenez pas Ã  Ãªtre dans le bon rÃ©pertoire.


## 3.1 Analyse d'un Ã©chantillon

**Remarques prÃ©alables** : 

- L'indexation du gÃ©nome de rÃ©fÃ©rence avec le logiciel `bowtie2` a dÃ©jÃ  Ã©tÃ© effectuÃ© pour vous. Pour vous en convraincre, affichez le contenu du rÃ©pertoire `/shared/projects/form_2021_29/data/rnaseq_tauri/genome` et vÃ©rifiez l'existence de fichiers avec l'extension `.bt2`, spÃ©cifiques des fichiers index crÃ©Ã©s par `bowtie2`.
- Cette indexation a Ã©tÃ© rÃ©alisÃ©e avec la commande `sbatch -A form_2021_29 /shared/projects/form_2021_29/data/rnaseq_tauri/build_genome_index.sh` qui, bien sÃ»r, vous n'exÃ©cutrez pas !

Depuis le cluster de l'IFB, vÃ©rifiez que vous Ãªtes toujours dans votre rÃ©pertoire `/shared/projects/form_2021_29/LOGIN/rnaseq_tauri`.

TÃ©lÃ©chargez ensuite le script `script4.sh` avec la commande :

```bash
$ wget https://raw.githubusercontent.com/omics-school/analyse-rna-seq/master/script4.sh
```

Lancez ce script avec la commande :

```bash
$ sbatch -A form_2021_29 script4.sh
```

Vous devriez obtenir un message du type `Submitted batch job 20716345`. Ici, `20716345` correspond au numÃ©ro du job.

Notez bien le numÃ©ro de votre job.

VÃ©rifiez que votre script est en train de tourner avec la commande :

```bash
$ squeue -u $USER
```

**Remarque** Voici quelques statuts (colonne `ST`) de job intÃ©ressant :

- `CA` (*cancelled*) : le job a Ã©tÃ© annulÃ©
- `F` (*failled*) : le job a plantÃ©
- `PD` (*pending*) : le job est en attente que des ressources soient disponibles
- `R` (*running*) : le job est lancÃ©


Et pour avoir plus de dÃ©tails, utilisez la commande :
```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```
avec `JOBID` (en fin de ligne) le numÃ©ro de votre job Ã  remplacer par le vÃ´tre.

Voici un exemple de sortie que vous pourriez obtenir :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j 20716345
       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList 
------------ ---------- ---------- ------------------- ---------- ---------- --------------- 
20716345     script4.sh    RUNNING 2022-01-05T23:37:36   00:08:58   00:08:58     cpu-node-24 
20716345.ba+      batch    RUNNING 2022-01-05T23:37:36   00:08:58   00:08:58     cpu-node-24 
20716345.0       fastqc    RUNNING 2022-01-05T23:37:37   00:00:54   00:00:34     cpu-node-24 
```

Si vous affichez le contenu de votre rÃ©pertoire courant, vous devriez voir l'apparition d'un fichier `slurm-JOBID.out` oÃ¹ `JOBID` est le numÃ©ro de votre job. Ce fichier contient la sortie, c'est-Ã -dire le *log* de votre script.

Affichez son contenu avec la commande `cat`. Par exemple :

```bash
$ cat slurm-JOBID.out
```

avec `JOBID` le numÃ©ro de votre job.

Si vous attendez 1 ou 2 minutes et relancez la commande `sacct` prÃ©cÃ©dente, votre job a du passer Ã  une nouvelle Ã©tape.

Par exemple :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j 20716345
       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList 
------------ ---------- ---------- ------------------- ---------- ---------- --------------- 
20716345     script4.sh    RUNNING 2022-01-05T23:37:36   00:05:22   00:05:22     cpu-node-24 
20716345.ba+      batch    RUNNING 2022-01-05T23:37:36   00:05:22   00:05:22     cpu-node-24 
20716345.0       fastqc  COMPLETED 2022-01-05T23:37:37   00:00:54   00:00:54     cpu-node-24 
20716345.1      bowtie2    RUNNING 2022-01-05T23:38:31   00:04:27   00:04:27     cpu-node-24 
```

Nous allons maintenant amÃ©liorer le script d'analyse. Annulez votre job en cours avec la commande :

```bash
$ scancel JOBID
```

oÃ¹ `JOBID` est le numÃ©ro de votre job.

Faites aussi un peu de mÃ©nage en supprimant les fichiers crÃ©Ã©s prÃ©cÃ©demment avec la commande :
```bash
$ rm -rf map/ reads_qc/ count/ slurm*.out
```

## 3.2 Analyse plus rapide d'un Ã©chantillon

L'objectif est maintenant Â« d'aller plus vite Â» en attribuant plusieurs coeurs pour l'Ã©tape d'alignement des reads sur le gÃ©nome avec `bowtie2`.

Toujours depuis le cluster de l'IFB, dans le rÃ©pertoire `rnaseq_tauri` de votre rÃ©pertoire de travail, tÃ©lÃ©chargez le script `script5.sh` avec la commande :

```bash
$ wget https://raw.githubusercontent.com/omics-school/analyse-rna-seq/master/script5.sh
```

Identifiez les diffÃ©rences avec le script prÃ©cÃ©dent, par exemple avec la commande `diff` :

```bash
$ diff script4.sh script5.sh
```

Les lignes qui dÃ©butent par `<` viennent de `script4.sh` et celles qui dÃ©butent par `>` viennent de `script5.sh`.

La diffÃ©rence majeure avec `script4.sh` rÃ©side dans l'utilisation de plusieurs coeurs pour la commande `bowtie2` avec l'option `--threads="${SLURM_CPUS_PER_TASK}"`. L'utilisation de plusieurs coeurs est permis par la dÃ©claration `#SBATCH --cpus-per-task=8` au tout dÃ©but de `script5.sh`.

**Remarque** : nous aurions Ã©galement pu attribuer plusieurs coeurs pour les commandes `samtools view` et `samtools sort`, mais nos tests ont montrÃ© qu'il n'y avait pas, pour ce cas prÃ©cis, de gain significatif en terme de temps de calcul. Pour information, les lignes de commande Ã  utiliser auraient Ã©tÃ© :

```bash
srun samtools view --threads="${SLURM_CPUS_PER_TASK}" -b "map/bowtie-${sample}.sam" -o "map/bowtie-${sample}.bam"
srun samtools sort --threads="${SLURM_CPUS_PER_TASK}" "map/bowtie-${sample}.bam" -o "map/bowtie-${sample}.sorted.bam"
```

Lancez maintenant le script d'analyse `script5.sh` :

```bash
$ sbatch -A form_2021_29 script5.sh
```

Notez bien le numÃ©ro de job renvoyÃ©.

VÃ©rifiez que votre job est bien lancÃ© avec la commande :
```bash
$ squeue -u $USER
```

Le fichier `slurm-JOBID.out` est Ã©galement crÃ©Ã© et contient les sorties du script. Pour consulter son contenu, tapez :

```bash
$ cat slurm-JOBID.out
```

avec `JOBID` le numÃ©ro de votre job.


Suivez Ã©galement en temps rÃ©el l'exÃ©cution de votre job avec la commande :

```bash
$ watch sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```
avec `JOBID` le numÃ©ro de votre job.

Remarques : 

- La commande `watch` est utilisÃ©e ici pour Â« surveiller Â» en quasi-temps rÃ©el le rÃ©sultat de la commande `sacct`.
- L'affichage est rafraichi toutes les 2 secondes.

Votre job devrait prendre une petite dizaine de minutes pour se terminer. Laissez le cluster travailler et profitez-en pour vous prÃ©parer un thÃ© ou un cafÃ© bien mÃ©ritÃ©.

Quand les status (colonne `State`) du job et de tous ses Â« *job steps* Â» sont Ã  `COMPLETED`, stoppez la commande `watch` en appuyant sur la combinaison de touches <kbd>Ctrl</kbd> + <kbd>C</kbd>.

VÃ©rifiez avec la commande `tree` que vous obtenez une arborescence Ã©quivalente Ã  celle ci-dessous :

```
$ tree
.
â”œâ”€â”€ count
â”‚   â””â”€â”€ count-SRR2960338.txt
â”œâ”€â”€ map
â”‚   â”œâ”€â”€ bowtie-SRR2960338.sorted.bam
â”‚   â””â”€â”€ bowtie-SRR2960338.sorted.bam.bai
â”œâ”€â”€ reads_qc
â”‚   â”œâ”€â”€ SRR2960338_fastqc.html
â”‚   â””â”€â”€ SRR2960338_fastqc.zip
â”œâ”€â”€ script4.sh
â”œâ”€â”€ script5.sh
â””â”€â”€ slurm-20716384.out
```

VÃ©rifiez que la somme de contrÃ´le du fichier `count/count-SRR2960338.txt` est bien `36fc86a522ee152c89fd77430e9b56a5`.

Faites maintenant un peu de mÃ©nage en supprimant les fichiers crÃ©Ã©s prÃ©cÃ©demment avec la commande :

```bash
$ rm -rf map/ reads_qc/ count/ slurm*.out
```



## 3.3 Analyse de plusieurs Ã©chantillons

Toujours depuis le cluster de l'IFB, dans le rÃ©pertoire `rnaseq_tauri` de votre rÃ©pertoire de travail, tÃ©lÃ©chargez le script `script6.sh` avec la commande :

```bash
$ wget https://raw.githubusercontent.com/omics-school/analyse-rna-seq/master/script6.sh
```

Nous pourrions analyser d'un seul coup les 47 Ã©chantillons (fichiers `.fastq.gz`) mais pour ne pas consommer trop de ressources sur le cluster, nous allons limiter notre analyse Ã  4 Ã©chantillons seulement. Si vous le souhaitez vous pourrez modifier ce script pour analyser les 47 Ã©chantillons ğŸ’ª.

Lancez votre analyse avec la commande :

```bash
$ sbatch -A form_2021_29 script6.sh
```

Notez bien le numÃ©ro de job renvoyÃ©.

Vous pouvez suivre en temps rÃ©el l'exÃ©cution de votre job avec la commande :

```bash
$ watch sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```
avec `JOBID` le numÃ©ro de votre job.

Remarquez que la ligne indiquant `script6.sh` pour Â« *JobName* Â» est prÃ©sente 4 fois. Cela indique que le script `script6.sh` est exÃ©cutÃ©e 4 fois, en parallÃ¨le.

Patientez une dizaine de minutes que tous les jobs et *job steps* soient terminÃ©s. 

Quand les status (colonne `State`) de tous les jobs et job steps sont Ã  `COMPLETED`, stoppez la commande `watch` en appuyant sur la combinaison de touches <kbd>Ctrl</kbd> + <kbd>C</kbd>.

Vous remarquerez que l'exÃ©cution de `script6.sh` aura pris environ le mÃªme temps que celle de `script5.sh`. C'est toute la puissance du calcule distribuÃ© ğŸš€ Vous comprenez qu'il est possible d'analyser 4, 10 ou 47 Ã©chantillons dans un temps raisonnable.

Une derniÃ¨re fois, vÃ©rifiez que tous vos fichiers sont prÃ©sents :

```bash
$ tree
.
â”œâ”€â”€ count
â”‚  â”œâ”€â”€ count-SRR2960338.txt
â”‚   â”œâ”€â”€ count-SRR2960341.txt
â”‚   â”œâ”€â”€ count-SRR2960343.txt
â”‚   â””â”€â”€ count-SRR2960356.txt
â”œâ”€â”€ map
â”‚   â”œâ”€â”€ bowtie-SRR2960338.sorted.bam
â”‚   â”œâ”€â”€ bowtie-SRR2960338.sorted.bam.bai
â”‚   â”œâ”€â”€ bowtie-SRR2960341.sorted.bam
â”‚   â”œâ”€â”€ bowtie-SRR2960341.sorted.bam.bai
â”‚   â”œâ”€â”€ bowtie-SRR2960343.sorted.bam
â”‚   â”œâ”€â”€ bowtie-SRR2960343.sorted.bam.bai
â”‚   â”œâ”€â”€ bowtie-SRR2960356.sorted.bam
â”‚   â””â”€â”€ bowtie-SRR2960356.sorted.bam.bai
â”œâ”€â”€ reads_qc
â”‚   â”œâ”€â”€ SRR2960338_fastqc.html
â”‚   â”œâ”€â”€ SRR2960338_fastqc.zip
â”‚   â”œâ”€â”€ SRR2960341_fastqc.html
â”‚   â”œâ”€â”€ SRR2960341_fastqc.zip
â”‚   â”œâ”€â”€ SRR2960343_fastqc.html
â”‚   â”œâ”€â”€ SRR2960343_fastqc.zip
â”‚   â”œâ”€â”€ SRR2960356_fastqc.html
â”‚   â””â”€â”€ SRR2960356_fastqc.zip
â”œâ”€â”€ script4.sh
â”œâ”€â”€ script5.sh
â”œâ”€â”€ script6.sh
â”œâ”€â”€ slurm-20716400_0.out
â”œâ”€â”€ slurm-20716400_1.out
â”œâ”€â”€ slurm-20716400_2.out
â””â”€â”€ slurm-20716400_3.out
```

Comme vous avez lancÃ© 4 sous-jobs indÃ©pendants, SLURM crÃ©e Ã©galement 4 fichiers de sortie distincts.

## 4. L'heure de faire les comptes

ExpÃ©rimentez la commande `sreport` pour avoir une idÃ©e du temps de calcul consommÃ© par tous vos jobs :

```bash
$ sreport -t hour Cluster UserUtilizationByAccount Start=2022-01-01 End=$(date --iso-8601)T23:59:59 Users=$USER
```

La colonne `Used` indique le nombre d'heures de temps CPU consommÃ©es. Cette valeur est utile pour estimer le Â« coÃ»t CPU Â» d'un projet.

Voici un exemple de rapport produit par `sreport` :

```bash
$ sreport -t hour Cluster UserUtilizationByAccount Start=2022-01-01 End=$(date --iso-8601)T23:59:59 Users=$USER
--------------------------------------------------------------------------------
Cluster/User/Account Utilization 2020-01-01T00:00:00 - 2021-03-18T21:59:59 (38268000 secs)
Usage reported in CPU Hours
--------------------------------------------------------------------------------
  Cluster     Login     Proper Name         Account     Used   Energy 
--------- --------- --------------- --------------- -------- -------- 
     core  ppoulain  Pierre Poulain       dubii2021      400        0 
     core  ppoulain  Pierre Poulain     du_bii_2019      246        0 
     core  ppoulain  Pierre Poulain uparis_duo_2020      129        0 
     core  ppoulain  Pierre Poulain        minomics        5        0 
```

Ainsi, l'utilisateur `ppoulain` a dÃ©jÃ  consommÃ© 129 heures de temps CPU sur le projet `uparis_duo_2020`.

Attention, `sreport` ne prend pas en compte les heures immÃ©diatement consommÃ©es. Il lui faut un peu de temps pour consolider les donnÃ©es.


## 5. RÃ©cupÃ©ration des donnÃ©es

### 5.1 scp

âš ï¸ Pour rÃ©cupÃ©rer des fichiers sur le cluster en ligne de commande, vous devez lancer la commande `scp` depuis un shell Unix sur votre machine locale. âš ï¸

Depuis un shell Unix sur votre machine locale, dÃ©placez-vous dans le rÃ©pertoire `/mnt/c/Users/omics` et crÃ©ez le rÃ©pertoire `rnaseq_cluster`. 

DÃ©placez-vous dans ce nouveau rÃ©pertoire.

Utilisez la commande `pwd` pour vÃ©rifier que vous Ãªtes bien dans le rÃ©pertoire `/mnt/c/Users/omics/rnaseq_cluster`. 

Lancez ensuite la commande suivante pour rÃ©cupÃ©rer les fichiers de comptage :

```bash
$ scp LOGIN@core.cluster.france-bioinformatique.fr:/shared/projects/form_2021_29/LOGIN/rnaseq_tauri/count/count*.txt .
```

oÃ¹ `LOGIN` est votre identifiant sur le cluster. Faites bien attention Ã  garder le `.` tout Ã  la fin de la commande.


VÃ©rifiez que la somme de contrÃ´le MD5 du fichier `count-SRR2960338.txt` est bien la mÃªme que prÃ©cÃ©demment (`36fc86a522ee152c89fd77430e9b56a5`).


### 5.2 Filezilla

Lancez le logiciel FileZilla ([comme ceci](img/filezilla.png)). Puis entrez les informations suivantes :

- HÃ´te : `sftp://core.cluster.france-bioinformatique.fr`
- Identifiant : votre login sur le cluster
- Mot de passe : votre mot de passe sur le cluster

Cliquez ensuite sur le bouton *Connexion rapide*. Cliquez sur *OK* dans la fenÃªtre *ClÃ© de l'hÃ´te inconnue*

Une fois connectÃ©, dans le champs texte Ã  cotÃ© de *Site distant* (Ã  droite de la fenÃªtre), entrez le chemin `/shared/projects/form_2021_29/` voire directement votre rÃ©pertoire de travail `/shared/projects/form_2021_29/LOGIN/` (avec `LOGIN` votre identifiant sur le cluster).

Essayez de transfÃ©rer des fichiers dans un sens puis dans l'autre. Double-cliquez sur les fichiers pour lancer les transferts.



